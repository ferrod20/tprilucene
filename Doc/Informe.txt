a, b y d:
Para realizar el tp elegí el SRI Lucene, en su versión para .Net (http://incubator.apache.org/lucene.net/). La elección de este sistema se basó prácticamente en que es un sistema bastante estable con una buena implementación y un amplio conjunto de funcionalidad. Además está disponible para .Net, siendo este último el lenguaje de mi dominio y preferencia.

Posibilidades del sistema:
El sistema permite elegir entre distintos Parsers, cada uno de ellos utilizando distintas estrategias de tokenización. Se describen en detalle más abajo.
También permite la implementación de stop words y asignación de peso al documento, al query y a los campos del documento.
Algunos de los parsers ya contienen stop words por defecto.

KeywordAnalyzer
Es un parser que toma el query completo como si fuera un unico token.

SimpleAnalyzer
Es un parser que convierte todo a minúscula. 

StopAnalyzer
Es un parser que convierte a minúscula y elimina todas las palabras definidas en el conjunto stop words. Palabras como articulos (a, an, the, etc) que tienen una ocurrencia comun en la mayoria de los lenguajes y que generan ruido en la busqueda.
StopAnalyzer contiene un conjunto de stop words por defecto, permitiendo también generar stop words particulares.

WhitespaceAnalyzer
Es un parser que genera los tokens partiendo por espacios en blanco.

StandardAnalyzer 
Es un parser que convierte a minúscula, elimina stop words y además intenta realizar uan limpieza de palabras, por ejemplo quitando los apóstrofes o eliminando los puntos; "U.B.A" se convierte en "UBA"

PerFieldAnalyzerWrapper
Es una composición de los demás parsers: posee la capacidad de utilizar distintos analizadores para distintos campos.
Por ejemplo, permite generar una configuración como la siguiente: utilizar StandardAnalyzer para todos los campos, excepto para los campos "nombre" y "apellido", para los cuales utilizar KeywordAnalyzer.


Mediciones
Se realizaron mediciones con cada uno de los Parsers utilizando el mismo parser tanto para indexar como para realizar la búsqueda.

Promedio precision: NeuN
Promedio recall: 0
Promedio R-Precision: 0
Promedio K-Precision: 0

Promedio precision: 0,0003822161
Promedio recall: 0,9535294
Promedio R-Precision: 0,273577
Promedio K-Precision: 0,1190476 


Promedio precision: 0,00141073
Promedio recall: 0,9054621
Promedio R-Precision: 0,2964229
Promedio K-Precision: 0,1238095
K=10


Promedio precision: < 0,0003
Promedio recall: 0,9134455
Promedio R-Precision: 0,1941017
Promedio K-Precision: 0,0822222


Promedio precision: 0,001270773
Promedio recall: 0,9054621
Promedio R-Precision: 0,3030052
Promedio K-Precision: 0,123492

El mejor resultado obtenido fue con el standardAnalyzer

3.

d)
La lista de stop words es: "a", "an", "and", "are", "as", "at", "be", "but", "by", "for", "if", "in", "into", "is", "it", "no", "not", "of", "on", "or", "such", "that", "the", "their", "then", "there", "these", "they", "this", "to", "was", "will", "with"
Es una lista en palabras comunes de idioma ingles que son poco relevantes para realizar busquedas, por lo tanto parece bastante adecuada para el corpus que manejamos.
Utilicé otro conjunto de stop words extendido con 300 palabras: "a","about","above","across","after","afterwards","again","against","all","almost","alone","along","already","also","although","always","am","among","amongst","amoungst","amount","an","and","another","any","anyhow","anyone","anything","anyway","anywhere","are","around","as","at","back","be","became","because","become","becomes","becoming","been","before","beforehand","behind","being","below","beside","besides","between","beyond","bill","both","bottom","but","by","call","can","cannot","cant","co","computer","con","could","couldnt","cry","de","describe","detail","do","done","down","due","during","each","eg","eight","either","eleven","else","elsewhere","empty","enough","etc","even","ever","every","everyone","everything","everywhere","except","few","fifteen","fify","fill","find","fire","first","five","for","former","formerly","forty","found","four","from","front","full","further","get","give","go","had","has","hasnt","have","he","hence","her","here","hereafter","hereby","herein","hereupon","hers","herself","him","himself","his","how","however","hundred","i","ie","if","in","inc","indeed","interest","into","is","it","its","itself","keep","last","latter","latterly","least","less","ltd","made","many","may","me","meanwhile","might","mill","mine","more","moreover","most","mostly","move","much","must","my","myself","name","namely","neither","never","nevertheless","next","nine","no","nobody","none","noone","nor","not","nothing","now","nowhere","of","off","often","on","once","one","only","onto","or","other","others","otherwise","our","ours","ourselves","out","over","own","part","per","perhaps","please","put","rather","re","same","see","seem","seemed","seeming","seems","serious","several","she","should","show","side","since","sincere","six","sixty","so","some","somehow","someone","something","sometime","sometimes","somewhere","still","such","system","take","ten","than","that","the","their","them","themselves","then","thence","there","thereafter","thereby","therefore","therein","thereupon","these","they","thick","thin","third","this","those","though","three","through","throughout","thru","thus","to","together","too","top","toward","towards","twelve","twenty","two","un","under","until","up","upon","us","very","via","was","we","well","were","what","whatever","when","whence","whenever","where","whereafter","whereas","whereby","wherein","whereupon","wherever","whether","which","while","whither","who","whoever","whole","whom","whose","why","will","with","within","without","would","yet","you","your","yours","yourself","yourselves"

El sistema posee la estrategia Porter stemmer, la misma se encuentra implementada en la clase PorterStemFilter.


c)
El sistema posee una única función de scoring, la cual se utiliza por defecto:

Dado un query q y un documento d, la función de scoring por defecto es:
score(q,d) = coord(q,d)*queryNorm(q)*sumatoria de los terminos t en el qeury q de( tf(t en d)*idf(t)2*peso(t)*norm(t,d) )

Donde: 
tf(t en d): raiz cuadrada de la frecuencia de t en d.
idf(t): 1+ log(numDocs/docFreq+1) donde numDocs es la cantidad total de documentos y docFreq es la cantidad de documentos que contienen el termino t.
coord(over,maxOver): over/maxOver donde over es el numero de terminos del query que encontrados en el documento y maxOver es el numero de terminos en el query
queryNorm(q): 1/sumatoria del peso del termino t para todos los terminos t en el query q.
norm(t,d): 1/raiz cuadrada del numero de terminos t que aparecen en d.

Implementacion de scoring function: 
Se realizó una prueba con una implementación de scoring function. Se implementaron los valores para tf e idf como sigue:
tf = 1+log(freq)
idf= max(0, log(ND-docFreq/docFreq) ) donde ND es la cantidad de documentos

